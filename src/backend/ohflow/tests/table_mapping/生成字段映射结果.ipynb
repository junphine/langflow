{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: 基于CPM-Bee进行字段匹配\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据格式处理 (Process dataset)\n",
    "训练之前，我们需要定义并处理我们的数据输入格式，我们构造一个数据集的处理类，将数据处理为特定格式。\n",
    "\n",
    "Before training, we need to define and process our data input format. We construct a processing class for the data set to process the data into a specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，我们使用的情感分类的输入格式如下（也可以自行定义其他格式）：\n",
    "\n",
    "In this tutorial, we use the following input format for emotion classification (you can also define other formats) :\n",
    "```\n",
    "数据表1: table_name_cn,table_name_en,table description\n",
    "字段1: field_name_cn,field_name_en,field description\n",
    "数据表2: table2\n",
    "字段2: field2\n",
    "\"options\": {\n",
    "      \"<option_0>\": \"表不匹配, 字段不匹配\", \n",
    "      \"<option_1>\": \"表匹配，字段不匹配\",\n",
    "      \"<option_2>\": \"表匹配，字段匹配\",\n",
    "    }, \n",
    "question: \"输入的数据表1与数据表2是否匹配，并且字段1与字段2是否匹配?\"\n",
    "<ans>: <option_0>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加工作路径\n",
    "\n",
    "Add working path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "random.seed(123)\n",
    "sys.path.append(\"../../src\")\n",
    "sys.path.append(\"/data/nlp/llm/CPM/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!pip install bminf\n",
    "#!wget --content-disposition https://cloud.tsinghua.edu.cn/f/bccfdb243eca404f8bf3/?dl=1\n",
    "#!tar -zxvf SST-2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm_live.tokenizers import CPMBeeTokenizer\n",
    "from cpm_live.training_tasks.bee import FinetuneDataset\n",
    "from cpm_live.models import CPMBeeConfig, CPMBeeTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import bmtrain as bmt\n",
    "from copy import deepcopy\n",
    "model_path = '/data/nlp/models/OpenBMB/cpm-bee-10b/'\n",
    "config = CPMBeeConfig.from_json_file(model_path+\"config.json\")\n",
    "ckpt_path = model_path+\"/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CPMBeeTokenizer()\n",
    "model = CPMBeeTorch(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('猫头鹰owl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(ckpt_path), strict=True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建dataloader\n",
    "build dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建runner\n",
    "\n",
    "Build runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cpm_live.generation.bee import CPMBeeBeamSearch\n",
    "data_list = [\n",
    "    {\"input1\": \"糖尿病该吃什么\",\"input2\": \"糖尿病人的食谱是什么\", \"prompt\": \"input1和input2是否语义一致？\",\"options\": {\n",
    "      \"<option_0>\": \"不一致\", \n",
    "      \"<option_1>\": \"同义\"      \n",
    "    },  \"<ans>\": \"\"},\n",
    "    {\"input\": \"NGC 6231是一个位于天蝎座的疏散星团，天球座标为赤经16时54分，赤纬-41度48分，视觉观测大小约45角分，亮度约2.6视星等，距地球5900光年。NGC 6231年龄约为三百二十万年，是一个非常年轻的星团，星团内的最亮星是5等的天蝎座 ζ1星。用双筒望远镜或小型望远镜就能看到个别的行星。NGC 6231在1654年被意大利天文学家乔瓦尼·巴蒂斯特·霍迪尔纳（Giovanni Battista Hodierna）以Luminosae的名字首次纪录在星表中，但是未见记载于夏尔·梅西耶的天体列表和威廉·赫歇尔的深空天体目录。这个天体在1678年被爱德蒙·哈雷（I.7）、1745年被夏西亚科斯（Jean-Phillippe Loys de Cheseaux）（9）、1751年被尼可拉·路易·拉卡伊（II.13）分别再次独立发现。\", \"question\": \"NGC 6231被哪些人发现过？\", \"<ans>\": \"\"}\n",
    "]\n",
    "# use beam search\n",
    "beam_search = CPMBeeBeamSearch(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "for data in data_list:\n",
    "    inference_results = beam_search.generate([data], max_length=100, repetition_penalty=1.1)\n",
    "    for res in inference_results:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 召回和匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "sys.path.append(\"/data/nlp/llm/CPM/\")\n",
    "sys.argv=['ipykernel_launcher.py',\n",
    "          '--delta','/data/nlp/llm/CPM/CPM-Bee/src/results/cpm_bee_finetune-delta-best.pt',\n",
    "          '--memory-limit','30',\n",
    "          '--device','cuda:0',\n",
    "          \"--use-bminf\"\n",
    "         ]\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import pymongo\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "from build_embedding_index import *\n",
    "import text_generation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = text_generation.parse_args()\n",
    "beam_search = text_generation.load_beam_search(args)\n",
    "model = beam_search.model\n",
    "tokenizer=beam_search.tokenizer\n",
    "model.eval()\n",
    "#if torch.cuda.device_count() > 1:\n",
    "#\tbeam_search.model= torch.nn.DataParallel(model,device_ids=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "生成匹配结果：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "header = 'ods_dataset_cn,ods_data_cn,std_dataset_cn,std_data_cn,score,pred_match,human_match'.split(',')\n",
    "\n",
    "PATH = r'C:/TEAM/贵州医药监管平台/'\n",
    "PATH = r'data/'\n",
    "\n",
    "db_uri = 'mongodb://172.16.29.84:2701/graph'\n",
    "mongo_client = pymongo.MongoClient(db_uri)\n",
    "db = mongo_client.get_default_database()\n",
    "\n",
    "collection_name = 'meta_info'\n",
    "collection = db[collection_name]\n",
    "\n",
    "std_dataset = collections.OrderedDict()\n",
    "with open(PATH+'标化模型.jsonl','r',encoding='utf-8') as fd:\n",
    "    for line in fd.readlines():\n",
    "        row = json.loads(line)\n",
    "        id = row['full_name_cn']\n",
    "        std_dataset[id] = row\n",
    "\n",
    "std_keys = list(std_dataset.keys())\n",
    "\n",
    "ods_dataset = read_ods_dataset(PATH+'贵州省医药监管平台ods模型.csv')\n",
    "\n",
    "#所有匹配的字段： std+ods:1\n",
    "matched_dict = {}\n",
    "matched_table_dict = {}\n",
    "matched_field_dict = {}\n",
    "result_dict = {}\n",
    "with open(PATH+'贵州省医药监管平台模型链路.jsonl','r',encoding='utf-8') as fd:\n",
    "    for line in fd.readlines():\n",
    "        row = json.loads(line)\n",
    "        row['data_name_cn'] = remove_number_around(row['data_name_cn'])\n",
    "        label = row['dataset_name_cn']+'.'+row['data_name_cn'] # ods\n",
    "        dwd_links = row['dwd_link'].split('\\n')\n",
    "\n",
    "        for dwd_link in dwd_links:\n",
    "            dwd_link = remove_brackets(dwd_link).strip(';').strip()\n",
    "            if dwd_link in std_dataset:\n",
    "                row_std = std_dataset[dwd_link]\n",
    "                matched_dict[dwd_link+':'+label]=1\n",
    "                matched_table_dict[row_std['dataset_name_cn']+':'+row['dataset_name_cn']]=1\n",
    "                matched_field_dict[row_std['data_name_cn']+':'+row['data_name_cn']]=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'卫生事件入口活动信息:门(急)诊挂号表': 1,\n",
       " '门(急)诊病历记录:门(急)诊病历': 1,\n",
       " '门(急)诊处方明细表:西药处方': 1,\n",
       " '门(急)诊处方主表:西药处方': 1,\n",
       " '收费单据明细表:门(急)诊结算记录': 1,\n",
       " '收费单据主表:门(急)诊结算记录': 1,\n",
       " '诊断明细表:入院诊断': 1,\n",
       " '医嘱记录明细表:住院医嘱': 1,\n",
       " '医嘱记录主表:住院医嘱': 1,\n",
       " '收费单据明细表:住院费用明细': 1,\n",
       " '收费单据主表:住院费用明细': 1,\n",
       " '收费单据主表:住院结算': 1,\n",
       " '出院记录:出院小结': 1,\n",
       " '住院病案首页基本信息:住院病案首页': 1,\n",
       " '住院病案首页诊断信息:住院病案首页': 1,\n",
       " '住院病案首页手术信息:住院病案首页': 1,\n",
       " '住院病案首页费用信息:住院病案首页': 1,\n",
       " '辅助检查报告主表:检查记录': 1,\n",
       " '检验报告单主表:检验记录': 1,\n",
       " '检验结果明细表:检验记录': 1,\n",
       " '入院记录:入院记录': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=30\n",
    "tp=1\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "c=0\n",
    "dataset = []\n",
    "# key:ods\n",
    "std_matched_dict = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 4\n",
    "if std_dataset:\n",
    "    for row2 in std_dataset.values():\n",
    "        if c>=0:\n",
    "            # std\n",
    "            if c>600:\n",
    "                b=3\n",
    "            row2 = build_text(row2)\n",
    "            query = row2['text']\n",
    "            query_en = row2['text_en']\n",
    "            label = row2['dataset_name_cn']+'.'+row2['data_name_cn'] # std\n",
    "            selector = dict(text_embedding={'$text':query,'$limit':k})\n",
    "            result = collection.find(selector)\n",
    "            query_results = collections.OrderedDict()\n",
    "            for doc in result:\n",
    "                id = doc['_id']\n",
    "                ods_row = ods_dataset[id]\n",
    "                text_embedding = doc['text_embedding']\n",
    "                distance = doc['_meta']['searchScore']\n",
    "                if distance<0.8:\n",
    "                    data = build_question(ods_row,row2)\n",
    "                    result_dict[label+':'+id] = build_ans(row2,ods_row,matched_dict,matched_field_dict,matched_table_dict)\n",
    "                    data[\"<ans>\"] = ''\n",
    "\n",
    "                    query_results[id]= data\n",
    "\n",
    "            selector = dict(text_en_embedding={'$text':query_en,'$limit':k})\n",
    "            result = collection.find(selector)\n",
    "            for doc in result:\n",
    "                id = doc['_id']\n",
    "                ods_row = ods_dataset[id]\n",
    "                text_embedding = doc['text_embedding']\n",
    "                distance = doc['_meta']['searchScore']\n",
    "                if distance<0.8:\n",
    "                    data = build_question(ods_row,row2)\n",
    "                    result_dict[label+':'+id] = build_ans(row2,ods_row,matched_dict,matched_field_dict,matched_table_dict)\n",
    "                    data[\"<ans>\"] = ''\n",
    "\n",
    "                    query_results[id]= data\n",
    "\n",
    "            if query_results:\n",
    "                found=0\n",
    "                i = 0                \n",
    "                labels = list(query_results.keys())\n",
    "                datas = list(query_results.values())\n",
    "                for index in range(0,len(datas),b):\n",
    "                    query = datas[index:index+b]\n",
    "                    inference_results = beam_search.generate(query, max_length=280, repetition_penalty=1)\n",
    "                    #inference_results = query\n",
    "                    for res in inference_results:\n",
    "                        id = labels[i]\n",
    "                        pred_label = res['<ans>']\n",
    "                        true_label = result_dict[label+':'+id]\n",
    "                        parts = id.split('.')\n",
    "                        score = res['<score>']\n",
    "                        if pred_label=='<option_3>':\n",
    "                            if true_label == pred_label:\n",
    "                                tp+=1\n",
    "                            else:\n",
    "                                fp+=1\n",
    "                            data = dict(std_dataset_cn=row2['dataset_name_cn'],std_data_cn=row2['data_name_cn'],\n",
    "                                        ods_dataset_cn=parts[0],ods_data_cn=parts[1],score=score,pred_match=pred_label,human_match=true_label)\n",
    "                            std_matched_dict[id].append(data)\n",
    "                            found+=1\n",
    "                        elif true_label=='<option_3>':\n",
    "                            if true_label != pred_label:\n",
    "                                fn+=1\n",
    "                                data = dict(std_dataset_cn=row2['dataset_name_cn'],std_data_cn=row2['data_name_cn'],\n",
    "                                            ods_dataset_cn=parts[0],ods_data_cn=parts[1],score=score,pred_match=pred_label,human_match=true_label)\n",
    "                                std_matched_dict[id].append(data)\n",
    "                            else:\n",
    "                                tn+=1\n",
    "                        else:\n",
    "                            tn+=1\n",
    "                        i+=1\n",
    "\n",
    "            c+=1\n",
    "            if c%100==0:\n",
    "                print('#tp='+str(tp))\n",
    "                print('#tn='+str(tn))\n",
    "                print('#fp='+str(fp))\n",
    "                print('#fn='+str(fn))\n",
    "                print('#total='+str(c))\n",
    "                print(f'#Precision={tp/(tp+fp)}')\n",
    "                print(f'#Recall={tp/(tp+fn)}')\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lends = len(dataset)\n",
    "print('tp='+str(tp))\n",
    "print('tn='+str(tn))\n",
    "print('fp='+str(fp))\n",
    "print('fn='+str(fn))\n",
    "print(f'Precision={tp/(tp+fp)}')\n",
    "print(f'Recall={tp/(tp+fn)}')\n",
    "print('total='+str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'predict_error.jsonl','a',encoding='utf-8') as fout:\n",
    "    for data in dataset:\n",
    "        json.dump(data,fout,ensure_ascii=False)\n",
    "        fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [       \n",
    "        {\"document\":\"老鼠凶狠地指着猫说：我现在和蝙蝠结婚了！将来我们的孩子生活在空中！再也不怕你了！猫哈哈大笑，指了指树上的猫头鹰说：看见没，这是俺<mask_1>！\",\n",
    "         \"<ans>\":\"\"},\n",
    "        {\"document\":\"老鼠凶狠地指着猫说：我现在和蝙蝠结婚了！将来我们的孩子生活在空中！再也不怕你了！猫哈哈大笑，指了指树上的<mask_0>说：看见没，这是俺<mask_1>！\",\n",
    "         \"<ans>\":{\"<mask_0>\": \"\",\"<mask_1>\": \"\"}}\n",
    "    ]\n",
    "beam_search.generate(data_list, max_length=256, repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [       \n",
    "        {\"address\":\"南岸窍角沱正街33号\",\"prompt\":\"将地址里面的省市县区镇村识别出来\",\n",
    "         \"<ans>\":\"\"},\n",
    "        {\"document\":\"老鼠凶狠地指着猫说：我现在和蝙蝠结婚了！将来我们的孩子生活在空中！再也不怕你了！猫哈哈大笑，指了指树上的<mask_0>说：看见没，这是俺<mask_1>！\",\n",
    "         \"<ans>\":{\"<mask_0>\": \"\",\"<mask_1>\": \"\"}}\n",
    "    ]\n",
    "beam_search.generate(data_list, max_length=256, repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=30\n",
    "tp=1\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "c=0\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(std_matched_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp=780\n",
      "tn=169437\n",
      "fp=1463\n",
      "fn=64\n",
      "Precision=0.34774855104770397\n",
      "Recall=0.9241706161137441\n",
      "total=2988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('tp='+str(tp))\n",
    "print('tn='+str(tn))\n",
    "print('fp='+str(fp))\n",
    "print('fn='+str(fn))\n",
    "print(f'Precision={tp/(tp+fp)}')\n",
    "print(f'Recall={tp/(tp+fn)}')\n",
    "print('total='+str(c))\n",
    "\n",
    "\n",
    "\n",
    "with open(PATH+'std_predict_result.csv','w',encoding='utf-8',newline='') as fout:\n",
    "    writer = csv.DictWriter(fout,header)\n",
    "    writer.writeheader()\n",
    "    for items in std_matched_dict.values():\n",
    "        writer.writerows(items) \n",
    "\n",
    "\n",
    "with open(PATH+'ods_predict_result.csv', 'w', encoding='utf-8',newline='') as f:\n",
    "    writer = csv.DictWriter(f,header)\n",
    "    writer.writeheader()\n",
    "    for row in ods_dataset.values():\n",
    "        label = row['dataset_name_cn']+'.'+row['data_name_cn']\n",
    "        if label in std_matched_dict:\n",
    "            writer.writerows(std_matched_dict[label])\n",
    "        else:\n",
    "            writer.writerow(dict(ods_dataset_cn=row['dataset_name_cn'],ods_data_cn=row['data_name_cn'],\n",
    "                                 std_dataset_cn='',std_data_cn='',pred_match='',human_match=''))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(std_matched_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=filter(lambda x: x.startswith('卫生事件'),list(matched_dict.keys()))\n",
    "for i in f:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([80098, 50085], {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('SCORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1={\"表1\": \"入院诊断,hospitalized diagnosis\", \"字段1\": \"入院记录id,admission id,关联入院记录\", \"表2\": \"入院记录,admission record\", \"字段2\": \"身份证件类别标准值,id type std,个体身份证件所属类别（如居民身份证、居民户口簿、护照等）在标准编码体系中的名称\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}\n",
    "doc2={\"表1\": \"住院医嘱,hospitalized order\", \"字段1\": \"药物类型编码,type code,药物类型代码cv5301.06\", \"表2\": \"医嘱记录主表,order record master\", \"字段2\": \"医嘱类别标准码,order type code std,临床医嘱类别（如临时、长期、出院带药等）在标准编码体系中的代码\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}\n",
    "doc3={\"表1\": \"住院医嘱,hospitalized order\", \"字段1\": \"药物类型编码,type code,药物类型代码cv5301.06\", \"表2\": \"医嘱记录主表,order record master\", \"字段2\": \"医疗机构原始编号,org id,医疗机构按照原始编码体系填写的唯一标识\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'表1': '入院诊断,hospitalized diagnosis',\n",
       "  '字段1': '入院记录id,admission id,关联入院记录',\n",
       "  '表2': '入院记录,admission record',\n",
       "  '字段2': '身份证件类别标准值,id type std,个体身份证件所属类别（如居民身份证、居民户口簿、护照等）在标准编码体系中的名称',\n",
       "  'options': {'<option_0>': '表不匹配,字段不匹配',\n",
       "   '<option_1>': '表匹配,字段不匹配',\n",
       "   '<option_2>': '表不匹配,字段匹配',\n",
       "   '<option_3>': '表匹配,字段匹配'},\n",
       "  'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）',\n",
       "  '<ans>': '<option_0>',\n",
       "  '<score>': -0.0034593939781188965},\n",
       " {'表1': '住院医嘱,hospitalized order',\n",
       "  '字段1': '药物类型编码,type code,药物类型代码cv5301.06',\n",
       "  '表2': '医嘱记录主表,order record master',\n",
       "  '字段2': '医嘱类别标准码,order type code std,临床医嘱类别（如临时、长期、出院带药等）在标准编码体系中的代码',\n",
       "  'options': {'<option_0>': '表不匹配,字段不匹配',\n",
       "   '<option_1>': '表匹配,字段不匹配',\n",
       "   '<option_2>': '表不匹配,字段匹配',\n",
       "   '<option_3>': '表匹配,字段匹配'},\n",
       "  'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）',\n",
       "  '<ans>': '<option_1>',\n",
       "  '<score>': -0.0008088350296020508},\n",
       " {'表1': '住院医嘱,hospitalized order',\n",
       "  '字段1': '药物类型编码,type code,药物类型代码cv5301.06',\n",
       "  '表2': '医嘱记录主表,order record master',\n",
       "  '字段2': '医疗机构原始编号,org id,医疗机构按照原始编码体系填写的唯一标识',\n",
       "  'options': {'<option_0>': '表不匹配,字段不匹配',\n",
       "   '<option_1>': '表匹配,字段不匹配',\n",
       "   '<option_2>': '表不匹配,字段匹配',\n",
       "   '<option_3>': '表匹配,字段匹配'},\n",
       "  'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）',\n",
       "  '<ans>': '<option_1>',\n",
       "  '<score>': -0.00010150671005249023}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search.generate([doc1,doc2,doc3], max_length=256, repetition_penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1={\"表1\": \"门(急)诊挂号表,registration record\", \"字段1\": \"挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别\", \"表2\": \"卫生事件入口活动信息,hevent entrance\", \"字段2\": \"常住户籍类型原始码,resident type code，个体的常住地址是否为户籍所在地类别在原始编码体系中的代码\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}\n",
    "doc2={\"表1\": \"门(急)诊挂号表,registration record\", \"字段1\": \"挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别\", \"表2\": \"卫生事件入口活动信息,hevent entrance\", \"字段2\": \"挂号途径原始码,registration path code,患者挂号途径（如现场、预约、特诊等）在原始编码体系中的代码\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}\n",
    "doc3={\"表1\": \"住院医嘱,hospitalized order\", \"字段1\": \"药物类型编码,type code,药物类型代码cv5301.06\", \"表2\": \"医嘱记录主表,order record master\", \"字段2\": \"医疗机构原始编号,org id,医疗机构按照原始编码体系填写的唯一标识\", \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"}, \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search.generate([doc1,doc2,doc3], max_length=280, repetition_penalty=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs, others= beam_search._process_list([doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(nan, device='cuda:0', dtype=torch.float16),\n",
       " tensor(nan, device='cuda:0', dtype=torch.float16),\n",
       " tensor(nan, device='cuda:0', dtype=torch.float16)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation2(beam_search,[doc1,doc2,doc3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=beam_search.tokenizer\n",
    "tokenizer.decode(pred_ids[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.decode(model_inputs[\"input\"][2].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二次检查\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import hashlib\n",
    "import re\n",
    "import csv\n",
    "test_dataset = []\n",
    "input_file = PATH+'ods_predict_result.csv'\n",
    "with open(input_file, 'r',encoding='utf-8') as f:\n",
    "    # 创建csv阅读器\n",
    "    reader = csv.DictReader(f)\n",
    "    # 遍历文件中的每一行\n",
    "    for row in reader:\n",
    "        \n",
    "        test_dataset.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=1\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "c=0\n",
    "import math\n",
    "\n",
    "results = collections.OrderedDict()\n",
    "if test_dataset:\n",
    "    for test_row in test_dataset:\n",
    "        if test_row['std_dataset_cn'] and test_row['pred_match']=='<option_3>': # and '常住户籍类型原始码'==test_row['std_data_cn']\n",
    "            # std\n",
    "            label_id = test_row['std_dataset_cn']+'.'+test_row['std_data_cn']\n",
    "            row2 = std_dataset[label_id]\n",
    "\n",
    "            #ods\n",
    "            id = test_row['ods_dataset_cn']+'.'+test_row['ods_data_cn']\n",
    "            ods_row = ods_dataset[id]\n",
    "\n",
    "            data = build_question(ods_row,row2)\n",
    "            true_label = build_ans(row2,ods_row,matched_dict,matched_field_dict,matched_table_dict)\n",
    "            pred_label = test_row['pred_match']\n",
    "            data[\"<ans>\"] = pred_label\n",
    "\n",
    "            query = [data]\n",
    "            inference_props = [test_row]\n",
    "            #inference_props = beam_search.generate(query, max_length=280, repetition_penalty=1)\n",
    "            for prop in inference_props:\n",
    "                parts = id.split('.')\n",
    "                score = float(prop['score'])\n",
    "                if '常住户籍类型原始码'==test_row['std_data_cn']:\n",
    "                    print('常住户籍类型原始码:')\n",
    "                    print(prop)\n",
    "                test_row['score'] = math.exp(score)\n",
    "                if score>-2:\n",
    "                    if true_label == pred_label:\n",
    "                        tp+=1\n",
    "                    else:\n",
    "                        fp+=1\n",
    "                    if id in results:\n",
    "                        results[id].append(test_row)\n",
    "                    else:\n",
    "                        results[id] = [test_row]\n",
    "\n",
    "                else:\n",
    "                    if true_label != pred_label:\n",
    "                        fn+=1\n",
    "                        print(test_row)\n",
    "                    else:\n",
    "                        tn+=1\n",
    "            c+=1\n",
    "            if c%100==0:\n",
    "                print('#tp='+str(tp))\n",
    "                print('#tn='+str(tn))\n",
    "                print('#fp='+str(fp))\n",
    "                print('#fn='+str(fn))\n",
    "                print('#total='+str(c))\n",
    "                print(f'#Precision={tp/(tp+fp)}')\n",
    "                print(f'#Recall={tp/(tp+fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999, 0.999000499833375)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-0.001,math.exp(-0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH+'ods_predict_matched_result_top_3.csv', 'w', encoding='utf-8',newline='') as fout:\n",
    "    writer = csv.DictWriter(fout,header)\n",
    "    writer.writeheader()\n",
    "    for items in results.values():\n",
    "        items.sort(key=lambda x: x['score'],reverse=True)\n",
    "        items = filter(lambda x: x['score']>0.98,items)\n",
    "        items = list(items)[0:3]\n",
    "        writer.writerows(items)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'表1': '门(急)诊挂号表,registration record', '字段1': '挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别', '表2': '卫生事件入口活动信息,hevent entrance', '字段2': '常住户籍类型原始码,resident type code，个体的常住地址是否为户籍所在地类别在原始编码体系中的代码', 'options': {'<option_0>': '表不匹配,字段不匹配', '<option_1>': '表匹配,字段不匹配', '<option_2>': '表不匹配,字段匹配', '<option_3>': '表匹配,字段匹配'}, 'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）', '<ans>': '<option_3>'}, {'表1': '门(急)诊挂号表,registration record', '字段1': '挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别', '表2': '卫生事件入口活动信息,hevent entrance', '字段2': '挂号途径原始码,registration path code,患者挂号途径（如现场、预约、特诊等）在原始编码体系中的代码', 'options': {'<option_0>': '表不匹配,字段不匹配', '<option_1>': '表匹配,字段不匹配', '<option_2>': '表不匹配,字段匹配', '<option_3>': '表匹配,字段匹配'}, 'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）', '<ans>': '<option_3>'}, {'表1': '住院医嘱,hospitalized order', '字段1': '药物类型编码,type code,药物类型代码cv5301.06', '表2': '医嘱记录主表,order record master', '字段2': '医疗机构原始编号,org id,医疗机构按照原始编码体系填写的唯一标识', 'options': {'<option_0>': '表不匹配,字段不匹配', '<option_1>': '表匹配,字段不匹配', '<option_2>': '表不匹配,字段匹配', '<option_3>': '表匹配,字段匹配'}, 'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）', '<ans>': '<option_1>'}, {'input': '狂风暴雨，今天天气是真的', 'prompt': '续写一段话', '<ans>': '怪'}]\n",
      "gen:ualiously，<option_2></s>\n",
      "gen:ualiously，<option_2></s>\n",
      "gen:ualiously，<option_0></s>\n",
      "gen:ualiously，好</s>\n",
      "[tensor(0.4255, device='cuda:0', dtype=torch.float16), tensor(0.3713, device='cuda:0', dtype=torch.float16), tensor(0.5000, device='cuda:0', dtype=torch.float16), tensor(0.0222, device='cuda:0', dtype=torch.float16)]\n"
     ]
    }
   ],
   "source": [
    "doc1={\"表1\": \"门(急)诊挂号表,registration record\", \"字段1\": \"挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别\",\n",
    "      \"表2\": \"卫生事件入口活动信息,hevent entrance\", \"字段2\": \"常住户籍类型原始码,resident type code，个体的常住地址是否为户籍所在地类别在原始编码体系中的代码\",\n",
    "      \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"},\n",
    "      \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"<option_0>\"}\n",
    "doc2={\"表1\": \"门(急)诊挂号表,registration record\", \"字段1\": \"挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别\",\n",
    "      \"表2\": \"卫生事件入口活动信息,hevent entrance\", \"字段2\": \"挂号途径原始码,registration path code,患者挂号途径（如现场、预约、特诊等）在原始编码体系中的代码\",\n",
    "      \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"},\n",
    "      \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"<option_2>\"}\n",
    "doc3={\"表1\": \"住院医嘱,hospitalized order\", \"字段1\": \"药物类型编码,type code,药物类型代码cv5301.06\",\n",
    "      \"表2\": \"医嘱记录主表,order record master\", \"字段2\": \"医疗机构原始编号,org id,医疗机构按照原始编码体系填写的唯一标识\",\n",
    "      \"options\": {\"<option_0>\": \"表不匹配,字段不匹配\", \"<option_1>\": \"表匹配,字段不匹配\", \"<option_2>\": \"表不匹配,字段匹配\", \"<option_3>\": \"表匹配,字段匹配\"},\n",
    "      \"question\": \"需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）\", \"<ans>\": \"<option_0>\"}\n",
    "doc4 = {\"input\": \"狂风暴雨，今天天气是真的\",\"prompt\":\"续写一段话\", \"<ans>\": \"\"}\n",
    "\n",
    "query=[doc1,doc2,doc3,doc4]\n",
    "inference_results = beam_search.generate(query, max_length=280, repetition_penalty=1)\n",
    "print(inference_results)\n",
    "inference_props = evaluation_props(beam_search,query)\n",
    "print(inference_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'表1': '门(急)诊挂号表,registration record',\n",
    "  '字段1': '挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别',\n",
    "  '表2': '卫生事件入口活动信息,hevent entrance',\n",
    "  '字段2': '常住户籍类型原始码，resident_type_code，个体的常住地址是否为户籍所在地类别在原始编码体系中的代码',\n",
    "  'options': {'<option_0>': '表不匹配,字段不匹配',\n",
    "   '<option_1>': '表匹配,字段不匹配',\n",
    "   '<option_2>': '表不匹配,字段匹配',\n",
    "   '<option_3>': '表匹配,字段匹配'},\n",
    "  'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）',\n",
    "}\n",
    " {'表1': '门(急)诊挂号表,registration record', \n",
    "  '字段1': '挂号类别代码,reg type code,【规则】ct05.10.004?挂号类别', \n",
    "  '表2': '卫生事件入口活动信息,hevent entrance', \n",
    "  '字段2': '常住户籍类型原始码,resident type code,个体的常住地址是否为户籍所在地类别在原始编码体系中的代码', \n",
    "  'options': {'<option_0>': '表不匹配,字段不匹配', '<option_1>': '表匹配,字段不匹配', '<option_2>': '表不匹配,字段匹配', '<option_3>': '表匹配,字段匹配'}, \n",
    "  'question': '需要把表2的字段2映射到表1的字段1，判断表1和表2,字段1和字段2是否匹配？（语义相似或者有包含关系则认为匹配）', '<ans>': '<option_3>'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "alpaca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
